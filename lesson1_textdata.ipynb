{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f67cc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš¨ Config not found for parakeet. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/auto_docstring.py\n",
      "ðŸš¨ Config not found for parakeet. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/auto_docstring.py\n",
      "ðŸš¨ Config not found for parakeet. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/auto_docstring.py\n"
     ]
    }
   ],
   "source": [
    "import re, sys, os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from plum import dispatch\n",
    "import torch\n",
    "import torch_directml\n",
    "from transformers import *\n",
    "from fastai.text.all import *\n",
    "from fastai.data.all import *\n",
    "import pandas as pd\n",
    "\n",
    "torch._logging.set_logs(all=logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b302fbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DIRECTML FULL DEBUG LOG STARTED\n",
      "torch version: 2.4.1+cpu\n",
      "Device: privateuseone:0\n",
      "GPU: Radeon RX 560X\u0000\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Setup DirectML\n",
    "print(\"=\"*80)\n",
    "print(\"DIRECTML FULL DEBUG LOG STARTED\")\n",
    "print(f\"torch version: {torch_directml.torch.__version__}\")\n",
    "print(f\"Device: {torch_directml.device()}\")\n",
    "print(f\"GPU: {torch_directml.device_name(0)}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "dml = torch_directml.device()\n",
    "\n",
    "# Global monkey-patch for Normalize\n",
    "old_init = Normalize.__init__\n",
    "def new_init(self, mean, std, axes=(0,2,3)):\n",
    "    old_init(self, mean.to(dml), std.to(dml), axes=axes)\n",
    "Normalize.__init__ = new_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2654e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.learner import Learner\n",
    "\n",
    "def directml_safe_freeze_to(self: Learner, n: int):\n",
    "    \"\"\"DirectML-safe freeze that works with HFModelWrapper\"\"\"\n",
    "    # For HFModelWrapper: model[0] = backbone, model[1] = classifier\n",
    "    if isinstance(self.model, HFModelWrapper):\n",
    "        if n == -1:  # Freeze everything except head\n",
    "            # Freeze the transformer backbone\n",
    "            for p in self.model.hf_model.base_model.parameters():\n",
    "                p.requires_grad_(False)\n",
    "            # Unfreeze the classifier head\n",
    "            if hasattr(self.model.hf_model, 'classifier'):\n",
    "                for p in self.model.hf_model.classifier.parameters():\n",
    "                    p.requires_grad_(True)\n",
    "        else:  # Unfreeze everything\n",
    "            for p in self.model.parameters():\n",
    "                p.requires_grad_(True)\n",
    "    else:\n",
    "        # Standard FastAI behavior for other models\n",
    "        if n == 0:\n",
    "            for p in self.model.parameters():\n",
    "                p.requires_grad_(True)\n",
    "        else:\n",
    "            backbone = self.model[0] if hasattr(self.model, '__getitem__') else self.model\n",
    "            backbone_ids = {id(p) for p in backbone.parameters()}\n",
    "            for p in backbone.parameters():\n",
    "                p.requires_grad_(False)\n",
    "            for p in self.model.parameters():\n",
    "                if id(p) not in backbone_ids:\n",
    "                    p.requires_grad_(True)\n",
    "    \n",
    "    # Don't recreate optimizer - DirectML issue\n",
    "    if self.opt is not None:\n",
    "        self.opt.clear_state()\n",
    "    \n",
    "    print(f\"âœ“ Freeze applied (n={n})\")\n",
    "\n",
    "# Patch the learner\n",
    "Learner.freeze_to = directml_safe_freeze_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dacd059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 1. Monkey-patch TensorText to add truncate and show methods\n",
    "# =====================================================\n",
    "@patch\n",
    "def truncate(self: TensorText, n):\n",
    "    \"\"\"Truncate TensorText to n tokens\"\"\"\n",
    "    return type(self)(self[:n])\n",
    "\n",
    "@patch  \n",
    "def show(self: TensorText, ctx=None, **kwargs):\n",
    "    \"\"\"Show decoded text\"\"\"\n",
    "    # Get the tokenizer from kwargs or use a stored one\n",
    "    tokenizer = kwargs.get('tokenizer', getattr(self, '_tokenizer', None))\n",
    "    \n",
    "    if tokenizer is not None:\n",
    "        # Decode using HF tokenizer\n",
    "        text = tokenizer.decode(self.cpu().tolist(), skip_special_tokens=True)\n",
    "    else:\n",
    "        # Fallback: just show token IDs\n",
    "        text = str(self.cpu().tolist())\n",
    "    \n",
    "    if ctx is None:\n",
    "        print(text)\n",
    "        return text\n",
    "    return show_title(text, ctx=ctx, **kwargs)\n",
    "\n",
    "# =====================================================\n",
    "# 2. Simple Transform - just tokenize to input_ids\n",
    "# =====================================================\n",
    "class HFTokenize(Transform):\n",
    "    \"\"\"Transform that tokenizes text files using HuggingFace tokenizer\"\"\"\n",
    "    def __init__(self, tokenizer, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        # Store tokenizer globally for TensorText.show() to access\n",
    "        TensorText._hf_tokenizer = tokenizer\n",
    "    \n",
    "    def encodes(self, path: Path):\n",
    "        \"\"\"Read file and tokenize, returning TensorText with just input_ids\"\"\"\n",
    "        text = path.read_text(encoding='utf-8') if path.exists() else \"\"\n",
    "        \n",
    "        # Tokenize - get tensors directly from tokenizer\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation='longest_first',\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            add_special_tokens=True,\n",
    "            return_tensors='pt'  # Return PyTorch tensors\n",
    "        )\n",
    "        \n",
    "        # Squeeze batch dimension [1, seq_len] -> [seq_len]\n",
    "        tensor_text = TensorText(encoding['input_ids'].squeeze(0))\n",
    "        # Attach tokenizer for later decoding\n",
    "        tensor_text._tokenizer = self.tokenizer\n",
    "        return tensor_text\n",
    "    \n",
    "    def decodes(self, o):\n",
    "        \"\"\"Keep as TensorText for show_batch compatibility\"\"\"\n",
    "        # Don't decode to string - keep as TensorText\n",
    "        # The actual text decoding happens in show() method\n",
    "        return o\n",
    "\n",
    "# =====================================================\n",
    "# 2. Callback that computes attention_mask on-the-fly\n",
    "# =====================================================\n",
    "class HFCallback(Callback):\n",
    "    \"\"\"\n",
    "    Computes attention_mask from padded input_ids and injects into model.\n",
    "    This is the ONLY place where HuggingFace-specific logic lives.\n",
    "    \"\"\"\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "    \n",
    "    def before_batch(self):\n",
    "        \"\"\"Compute attention_mask from input_ids before each forward pass\"\"\"\n",
    "        # Get input_ids from batch\n",
    "        xb = self.xb[0] if isinstance(self.xb, tuple) and len(self.xb) > 0 else self.xb\n",
    "        \n",
    "        # Compute attention_mask: 1 for real tokens, 0 for padding\n",
    "        attention_mask = (xb != self.pad_idx).long()\n",
    "        \n",
    "        # Inject into model for this forward pass\n",
    "        self.model._attention_mask = attention_mask\n",
    "\n",
    "# =====================================================\n",
    "# 3. Model wrapper that uses injected attention_mask\n",
    "# =====================================================\n",
    "class HFModelWrapper(Module):\n",
    "    \"\"\"Wrapper to adapt HF model to FastAI\"\"\"\n",
    "    def __init__(self, hf_model, pad_idx):\n",
    "        self.hf_model = hf_model\n",
    "        self.pad_idx = pad_idx\n",
    "        self._attention_mask = None\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        # Get attention_mask from callback (or compute default)\n",
    "        attention_mask = getattr(self, '_attention_mask', None)\n",
    "        if attention_mask is None:\n",
    "            attention_mask = (input_ids != self.pad_idx).long()\n",
    "        \n",
    "        # Clear after use (for next batch)\n",
    "        self._attention_mask = None\n",
    "        \n",
    "        # Forward pass with attention mask\n",
    "        outputs = self.hf_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        return outputs.logits\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Make model subscriptable for FastAI's freeze_to\"\"\"\n",
    "        if idx == 0:\n",
    "            # Return the transformer backbone (everything except classifier)\n",
    "            return self.hf_model.base_model\n",
    "        else:\n",
    "            # Return the classifier head\n",
    "            return self.hf_model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cc74957",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at models\\models--haisongzhang--roberta-tiny-cased\\snapshots\\2e8caf4404987b8cb20fa4b22955f56940b2ebc6\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "Attempting to create safetensors variant\n",
      "Safetensors PR exists\n",
      "loading weights file model.safetensors from cache at models\\models--haisongzhang--roberta-tiny-cased\\snapshots\\bfd314bd7663eef65c83e57719c0d9f3c21aa631\\model.safetensors\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at haisongzhang/roberta-tiny-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file config.json from cache at C:\\Users\\Keshav\\.cache\\huggingface\\hub\\models--haisongzhang--roberta-tiny-cased\\snapshots\\2e8caf4404987b8cb20fa4b22955f56940b2ebc6\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\Keshav\\.cache\\huggingface\\hub\\models--haisongzhang--roberta-tiny-cased\\snapshots\\2e8caf4404987b8cb20fa4b22955f56940b2ebc6\\vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\Keshav\\.cache\\huggingface\\hub\\models--haisongzhang--roberta-tiny-cased\\snapshots\\2e8caf4404987b8cb20fa4b22955f56940b2ebc6\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\Keshav\\.cache\\huggingface\\hub\\models--haisongzhang--roberta-tiny-cased\\snapshots\\2e8caf4404987b8cb20fa4b22955f56940b2ebc6\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\Keshav\\.cache\\huggingface\\hub\\models--haisongzhang--roberta-tiny-cased\\snapshots\\2e8caf4404987b8cb20fa4b22955f56940b2ebc6\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\Keshav\\.cache\\huggingface\\hub\\models--haisongzhang--roberta-tiny-cased\\snapshots\\2e8caf4404987b8cb20fa4b22955f56940b2ebc6\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 5. Setup DataLoaders and Model\n",
    "# =====================================================\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "model_cache_dir = Path(\"models\")\n",
    "hf_model = \"haisongzhang/roberta-tiny-cased\"\n",
    "\n",
    "# Load HF model and tokenizer\n",
    "hf_model_obj = AutoModelForSequenceClassification.from_pretrained(\n",
    "    hf_model, num_labels=2, cache_dir=model_cache_dir,\n",
    "    use_safetensors=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_model)\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "704b566e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "path = untar_data(URLs.IMDB)\n",
    "\n",
    "# Create DataBlock with HFTransform\n",
    "blocks = (\n",
    "    TransformBlock(\n",
    "        type_tfms=HFTokenize(tokenizer, max_length=512),\n",
    "        dls_kwargs={'before_batch': Pad_Chunk(seq_len=512, pad_idx=tokenizer.pad_token_id)}\n",
    "    ),\n",
    "    CategoryBlock\n",
    ")\n",
    "\n",
    "dblock = DataBlock(\n",
    "    blocks=blocks,\n",
    "    get_items=get_text_files,\n",
    "    get_y=parent_label,\n",
    "    splitter=GrandparentSplitter(valid_name='test')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a622ffe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting items from C:\\Users\\Keshav\\.fastai\\data\\imdb\n",
      "Found 100002 items\n",
      "2 datasets of sizes 25000,25000\n",
      "Setting up Pipeline: HFTokenize\n",
      "Setting up Pipeline: parent_label -> Categorize -- {'vocab': None, 'sort': True, 'add_na': False}\n",
      "\n",
      "Setting up after_item: Pipeline: ToTensor\n",
      "Setting up before_batch: Pipeline: Pad_Chunk -- {'pad_idx': 0, 'pad_first': True, 'seq_len': 512}\n",
      "\n",
      "Setting up after_batch: Pipeline: \n"
     ]
    }
   ],
   "source": [
    "# Create dataloaders\n",
    "dls = dblock.dataloaders(path, bs=1, device=dml, verbose=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "911caa5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>None</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>well when watching this film late one night i was simple amazed by it's greatness. fantastic script, great acting, costumes and special effects, and the plot twists, wow!! in fact if you can see the ending coming you should become a writer yourself. &lt; br / &gt; &lt; br / &gt; great, i would recommend this film to anyone, especially if i don ; t like them much. &lt; br / &gt; &lt; br / &gt; terrific</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12ce1d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LEARNER CREATED SUCCESSFULLY\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 7. Create Learner with custom callback\n",
    "# =====================================================\n",
    "# Wrap the HF model\n",
    "model_wrapped = HFModelWrapper(hf_model_obj, tokenizer.pad_token_id)\n",
    "model_wrapped.to(dml)\n",
    "\n",
    "# Create learner\n",
    "learn = Learner(\n",
    "    dls, \n",
    "    model_wrapped,\n",
    "    loss_func=CrossEntropyLossFlat(),\n",
    "    metrics=accuracy,\n",
    "    cbs=[HFCallback(pad_idx=tokenizer.pad_token_id)]\n",
    ").to_fp16(enabled=False)\n",
    "\n",
    "learn.to(dml)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LEARNER CREATED SUCCESSFULLY\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6e093f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "VERIFYING BATCH STRUCTURE\n",
      "================================================================================\n",
      "Batch shape: torch.Size([1, 512])\n",
      "Batch dtype: torch.int64\n",
      "First item type: <class 'fastai.text.data.TensorText'>\n",
      "Labels shape: torch.Size([1])\n",
      "Pad token ID: 0\n",
      "Sample tokens (first 20): TensorText([  101,  1175,  1110,  7284,  1720,  1106,  1894,  3051,  1306,\n",
      "             1142,  2523,   119,  1152,  1261,   170,   188, 19094,  6482,\n",
      "             1642,   117], device='privateuseone:0')\n",
      "Padding tokens in first sample: 276/512\n",
      "\n",
      "âœ“ Batch structure correct!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VERIFYING BATCH STRUCTURE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    batch = dls.one_batch()\n",
    "    xb, yb = batch\n",
    "    \n",
    "    print(f\"Batch shape: {xb.shape}\")\n",
    "    print(f\"Batch dtype: {xb.dtype}\")\n",
    "    print(f\"First item type: {type(xb[0])}\")\n",
    "    print(f\"Labels shape: {yb.shape}\")\n",
    "    print(f\"Pad token ID: {tokenizer.pad_token_id}\")\n",
    "    print(f\"Sample tokens (first 20): {xb[0][:20]}\")\n",
    "    \n",
    "    # Show padding\n",
    "    pad_count = (xb[0] == tokenizer.pad_token_id).sum().item()\n",
    "    print(f\"Padding tokens in first sample: {pad_count}/{len(xb[0])}\")\n",
    "    \n",
    "    print(\"\\nâœ“ Batch structure correct!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâœ— Batch verification error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7916a483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =====================================================\n",
    "# # 9. Test forward pass with attention_mask injection\n",
    "# # =====================================================\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(\"TESTING FORWARD PASS WITH ATTENTION MASK\")\n",
    "# print(\"=\"*80)\n",
    "\n",
    "# try:\n",
    "#     with torch.no_grad():\n",
    "#         batch = dls.one_batch()\n",
    "#         learn.xb, learn.yb = batch\n",
    "        \n",
    "#         # Find our HFCallback\n",
    "#         hf_callback = None\n",
    "#         for cb in learn.cbs:\n",
    "#             if isinstance(cb, HFCallback):\n",
    "#                 hf_callback = cb\n",
    "#                 break\n",
    "        \n",
    "#         print(f\"Found HFCallback: {hf_callback is not None}\")\n",
    "        \n",
    "#         # Show what callback does\n",
    "#         print(\"\\nBefore callback:\")\n",
    "#         print(f\"  Model has _attention_mask: {hasattr(learn.model, '_attention_mask')}\")\n",
    "#         print(f\"  Input batch shape: {learn.xb[0].shape}\")\n",
    "#         print(f\"  Input batch type: {type(learn.xb[0])}\")\n",
    "        \n",
    "#         # Trigger callback manually\n",
    "#         if hf_callback:\n",
    "#             hf_callback.before_batch()\n",
    "            \n",
    "#             print(\"\\nAfter callback:\")\n",
    "#             print(f\"  Model has _attention_mask: {hasattr(learn.model, '_attention_mask')}\")\n",
    "#             if hasattr(learn.model, '_attention_mask'):\n",
    "#                 print(f\"  Attention mask shape: {learn.model._attention_mask.shape}\")\n",
    "#                 print(f\"  Sample attention mask (first 30): {learn.model._attention_mask[0][:30]}\")\n",
    "#                 print(f\"  Num real tokens: {learn.model._attention_mask[0].sum().item()}\")\n",
    "#                 print(f\"  Num pad tokens: {(learn.model._attention_mask[0] == 0).sum().item()}\")\n",
    "        \n",
    "#         # Forward pass - pass the whole batch, not just first element\n",
    "#         input_tensor = learn.xb[0] if isinstance(learn.xb, tuple) else learn.xb\n",
    "#         pred = learn.model(input_tensor)\n",
    "        \n",
    "#         print(\"\\nForward pass results:\")\n",
    "#         print(f\"  Input shape: {input_tensor.shape}\")\n",
    "#         print(f\"  Output shape: {pred.shape}\")\n",
    "#         print(f\"  Sample logits: {pred[0]}\")\n",
    "#         print(\"\\nâœ“ Forward pass successful!\")\n",
    "        \n",
    "# except Exception as e:\n",
    "#     print(f\"\\nâœ— Forward pass error: {e}\")\n",
    "#     import traceback\n",
    "#     traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a8b464d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Freeze applied (n=-1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Projects\\VsCodium\\Python\\2025\\GenAI\\11\\.venv3_12_9\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:265: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "f:\\Projects\\VsCodium\\Python\\2025\\GenAI\\11\\.venv3_12_9\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.439975</td>\n",
       "      <td>0.441304</td>\n",
       "      <td>0.795480</td>\n",
       "      <td>1:43:13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Freeze applied (n=0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.479586</td>\n",
       "      <td>0.422859</td>\n",
       "      <td>0.804920</td>\n",
       "      <td>2:48:38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fine_tune(1, base_lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b4e68a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Projects\\VsCodium\\Python\\2025\\GenAI\\11\\.venv3_12_9\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:265: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "f:\\Projects\\VsCodium\\Python\\2025\\GenAI\\11\\.venv3_12_9\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('pos', TensorText(1), TensorText([0.1503, 0.8497]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_path = Path(\"test_reviews/test_1.txt\")\n",
    "learn.predict(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4557f0f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv3_12_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
